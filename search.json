[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "FastAI Journey Status\n\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lecture 02\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nHistory\n\n\nNeural Network\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lecture 02\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nHistory\n\n\nNeural Network\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lecture 01\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nHistory\n\n\nNeural Network\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Snippets 01\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nUtils\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Setup; Understanding Tools\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nTools\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Notes\n\n\n\n\n\n\n\nnotes\n\n\nFastAI\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Journey\n\n\n\n\n\n\n\nFolder\n\n\nDeepLearning\n\n\nFastAI\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nQuarto ipynb blog test\n\n\n\n\n\n\n\ntest\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Notes/FastAI/index.html",
    "href": "posts/Notes/FastAI/index.html",
    "title": "FastAI Notes",
    "section": "",
    "text": "This is a post with executable code.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\nDate\n\n\n\n\n\n\nFastAI Lecture 01\n\n\n\n\n\nDec 31, 2023\n\n\n\n\nFastAI Lecture 02\n\n\n\n\n\nJan 11, 2024\n\n\n\n\nFastAI Lecture 02\n\n\n\n\n\nJan 13, 2024\n\n\n\n\nFastAI Setup; Understanding Tools\n\n\n\n\n\nDec 29, 2023\n\n\n\n\nFastAI Snippets 01\n\n\n\n\n\nDec 31, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Setup.html",
    "href": "posts/Notes/FastAI/FastAI Setup.html",
    "title": "FastAI Setup; Understanding Tools",
    "section": "",
    "text": "We’ll be taking a look at package managers like conda, mamba; peek on some important tools like bash, tmux etc."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Setup.html#setting-up-windows-subsystem",
    "href": "posts/Notes/FastAI/FastAI Setup.html#setting-up-windows-subsystem",
    "title": "FastAI Setup; Understanding Tools",
    "section": "Setting up Windows Subsystem",
    "text": "Setting up Windows Subsystem\n\nSet wsl\nget the miniforge/mambaforge(deprected)\nrun the miniforge.sh\nmamba install ipython\nmamba install jupyter lab\nmamba install pytorch\nconda install fastai"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Setup.html#nvidia-smi",
    "href": "posts/Notes/FastAI/FastAI Setup.html#nvidia-smi",
    "title": "FastAI Setup; Understanding Tools",
    "section": "nvidia-smi",
    "text": "nvidia-smi\nnvidia-smi dmon: Chec for sm and mem column ## Jupter %%debug : Used for non-graphical debugging from pdb import set_trace : set_trace() sets a breakpoint and automatically enables debugger mode ## Symblinks ### Linux ln -s target-path : Creates a symblink of the target directory in current directory ln -la : Displays the folders who are made up of symblink\n\nWindows\nYoutube Tutorial"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Setup.html#cloud-environments",
    "href": "posts/Notes/FastAI/FastAI Setup.html#cloud-environments",
    "title": "FastAI Setup; Understanding Tools",
    "section": "Cloud Environments",
    "text": "Cloud Environments\n\nPaperspace\nQ. How to set common library? 1. pip install --user with this flag so that the lib is installed in .local 2. move the .local to /storage/.local 3. create symblink from /storage/.local as target and it would create .bash.local in storage which would fire up time new instance has been called"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 02.html",
    "href": "posts/Notes/FastAI/FastAI Lecture 02.html",
    "title": "FastAI Lecture 02",
    "section": "",
    "text": "Dataloaders is a container which contains the data (train and validation) we want to use to for our machine learning model; it organizes the data.\nDataLoaders class is the class that passes the data to the fastai model. It is essentially a class that required Dataloader objects (usually for train and validation subset)\n\nitem_tfms: Our images are all different sizes, and this is a problem for deep learning: we don’t feed the model one image at a time but several of them (what we call a mini-batch). To group them in a big array (usually called a tensor) that is going to go through our model, they all need to be of the same size. So, we need to add a transform which will resize these images to the same size. Item transforms are pieces of code that run on each individual item, whether it be an image, category, or so forth. fastai includes many predefined transforms; we use the Resize transform here:\nDataBlock: This command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data—in this case, the path where the images can be found\nA DataLoaders includes validation and training DataLoaders. DataLoader is a class that provides batches of a few items at a time to the GPU. We’ll be learning a lot more about this class in the next chapter. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader: dls.valid.show_batch(max_n=4, nrows=1) dls is a dataloaders dls.valid and dls.train is dataloader ## Cropping - Resize(size): crops the image to fit a square of the size requested, using the full width or height. - ResizeMethod.Squish: can result in loosing of some detail as images gets squished or stretched. - ResizeMethod.Pad, pad_mode=‘zero’: Fits the image in square with black padding. These approaches are unoptimized and result in lower accurate models. We can train our model on random parts of image during each training round (EPOCH), making it adaptable to different features, similar to how photos can have different perspective. It helps the model to generalize the feature and learn the basic concept of object - RandomResizedCrop(size, min_scale=0.3): Creates random zoom images during epoch in vram dls.train.show_batch(max_n=4, nrows=1, unique=True) We used unique=True to have the same image repeated with different versions of this RandomResizedCrop transform. This is a specific example of a more general technique, called data augmentation. ## Data Augmentation With resize our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 02.html#dataloaders",
    "href": "posts/Notes/FastAI/FastAI Lecture 02.html#dataloaders",
    "title": "FastAI Lecture 02",
    "section": "",
    "text": "Dataloaders is a container which contains the data (train and validation) we want to use to for our machine learning model; it organizes the data.\nDataLoaders class is the class that passes the data to the fastai model. It is essentially a class that required Dataloader objects (usually for train and validation subset)\n\nitem_tfms: Our images are all different sizes, and this is a problem for deep learning: we don’t feed the model one image at a time but several of them (what we call a mini-batch). To group them in a big array (usually called a tensor) that is going to go through our model, they all need to be of the same size. So, we need to add a transform which will resize these images to the same size. Item transforms are pieces of code that run on each individual item, whether it be an image, category, or so forth. fastai includes many predefined transforms; we use the Resize transform here:\nDataBlock: This command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data—in this case, the path where the images can be found\nA DataLoaders includes validation and training DataLoaders. DataLoader is a class that provides batches of a few items at a time to the GPU. We’ll be learning a lot more about this class in the next chapter. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader: dls.valid.show_batch(max_n=4, nrows=1) dls is a dataloaders dls.valid and dls.train is dataloader ## Cropping - Resize(size): crops the image to fit a square of the size requested, using the full width or height. - ResizeMethod.Squish: can result in loosing of some detail as images gets squished or stretched. - ResizeMethod.Pad, pad_mode=‘zero’: Fits the image in square with black padding. These approaches are unoptimized and result in lower accurate models. We can train our model on random parts of image during each training round (EPOCH), making it adaptable to different features, similar to how photos can have different perspective. It helps the model to generalize the feature and learn the basic concept of object - RandomResizedCrop(size, min_scale=0.3): Creates random zoom images during epoch in vram dls.train.show_batch(max_n=4, nrows=1, unique=True) We used unique=True to have the same image repeated with different versions of this RandomResizedCrop transform. This is a specific example of a more general technique, called data augmentation. ## Data Augmentation With resize our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time."
  },
  {
    "objectID": "posts/FastAI/Status.html",
    "href": "posts/FastAI/Status.html",
    "title": "FastAI Journey Status",
    "section": "",
    "text": "Live Coding:\nCompleted : 1,2,3,6,7\n\n\n\nLesson\n\n✅ Done\n🌀 Current Sprint\n💤 Pending\n\n\n\n\n\n\n\n\n\n\n\n\nLesson\nWatched\nRead\nQuestionnaire\nRe-Implementation\nSelf-Implementation\n\n\n\n\n1\n✅\n✅\n✅\n✅\n🌀\n\n\n2\n✅\n✅\n🌀\n✅\n💤\n\n\n3\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n5"
  },
  {
    "objectID": "notesIndex.html",
    "href": "notesIndex.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 29, 2023\n\n\nFastAI Notes\n\n\nAgastya Patel\n\n\n\n\nDec 29, 2023\n\n\nFastAI Setup; Understanding Tools\n\n\nAgastya Patel\n\n\n\n\nDec 31, 2023\n\n\nFastAI Snippets 01\n\n\nAgastya Patel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Agastya. Welcome to my second brain which I share over internet.\nI’m currently working as Technical Artist and Developer. My responsibility involves developing custom in house solutions also whilst leveraging power of open source models for the creative team. I’m currently working on shot tracking product with the cloud team.\nMy interst in deep learning has been growing over me since I started using open source models which have lead into learning about it. I believe in constant learning and hence this is a good platform for sharing what I’m currently working/learning about.\nTech Stack\n- Python, Javascript, C++, SQL, HTML -\n- Framework/libs FastAI, PyQT, Flask, DCC Based APIs -\n- DCC: Blender, Unreal, Houdini, Maya -"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "FastAI Journey Status\n\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lecture 02\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nHistory\n\n\nNeural Network\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lecture 02\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nHistory\n\n\nNeural Network\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lecture 01\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nHistory\n\n\nNeural Network\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Snippets 01\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nUtils\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Setup; Understanding Tools\n\n\n\n\n\n\n\nNotes\n\n\nFastAI\n\n\nTools\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Notes\n\n\n\n\n\n\n\nnotes\n\n\nFastAI\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Journey\n\n\n\n\n\n\n\nFolder\n\n\nDeepLearning\n\n\nFastAI\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\n  \n\n\n\n\nQuarto ipynb blog test\n\n\n\n\n\n\n\ntest\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nAgastya Patel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/FastAI/index.html",
    "href": "posts/FastAI/index.html",
    "title": "FastAI Journey",
    "section": "",
    "text": "Here’s my current journey of Fast AI learning. I’ll be keeping tab of the projects and the current status of journey here!.\nMy FastAI/Course Journey: github\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\nDate\n\n\n\n\n\n\nFastAI Journey Status\n\n\n\n\n\nJan 15, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html",
    "title": "FastAI Lecture 01",
    "section": "",
    "text": "Useful Resources This is how I found out about fastAI :) Alliance MAP JH’s jupyter NB 101 Kaggle: Comprehensive data exploration with Python”\nLesson1 Notes : 07/12/2023 - learn about functional programming - What is fine tuning?"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#fastai-api",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#fastai-api",
    "title": "FastAI Lecture 01",
    "section": "FastAI API",
    "text": "FastAI API\n\nDatablocks & DataLoaders\nDatablocks are template which are used for creating DataLoaders (FastAI) based on DataLoader (PyTorch)\nDataLoader class creates object which makes generally makes two sets ie, train and valid sets. To convert our data into DataLoader. We need - To know what kinds of data we are working with ie, input data type and result data type. - how to get the list of the item - how to get the labels of the item - how to create validation set Here, Datablock Provides the functionality to add all these information.\nDataBlock-&gt;DataLoader-&gt;ModelTraining\n# Get images from a folder (recursively)\nget_image_files (path, recurse=True, folders=None)"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#history-of-neural-network",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#history-of-neural-network",
    "title": "FastAI Lecture 01",
    "section": "History of Neural Network",
    "text": "History of Neural Network\n\nWarren McCulloch, Walter Pitts: Developed mathematical model of artificial neuron. Logical Calculus of the ideas immanent in Nervous Activity ;Defined “all-or-none” characteristic of neuron; can be represented using simple addition and thresholding.\nRosenblatt Built artificial neuron - ‘Mark I Perceptron’ using above principles; Machine capable of recognizing and identifying its surrounding (Simple features) without human training and supervision. ### AI Winter #1\nMarvin Minsky wrote a book ‘Perceptron’ and conveyed that single layer of these devices were unable to learn simple mathematical function like XOR. But also proposed that use of multiple layers which would remove this limitation.\nWorld looked at the first of his outcome and research depleted for 2 decades.\nParallel Distribution Processing’s idea of using computational framework for modeling cognitive processing similar to brain. #### PDP’s approach still being used today in creating modern neural networks A set of processing units state of activation output function for each unit/node pattern of connectivity, rule for propagating patterns of activity in the network activation rule for combining the inputs at a node with current state to produce output for that node learning rule where connection patterns are modified with experience environment where system can operate ### AI Winter #2\nWith PDP’s approach, researchers started adding another layer in network but due to being complex, large and slow in nature; the field got deserted. ### Today\nMultiple layers of networks are being built due to technological advancements, data availability and better algorithm. The performance has increased drastically.\n\n\nPaper ^50k citations written by Alec Radford “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” [READ PENDING] Musk - “https://twitter.com/elonmusk/status/1224089444963311616”\n\n\nGPU (Graphics Card) - Special kind of processors that handles thousands of single task at the same time. These tasks are similar to neural networks do, such that GPU’’s run neural network much faster than CPU"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#what-is-machine-learning",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#what-is-machine-learning",
    "title": "FastAI Lecture 01",
    "section": "What is machine learning?",
    "text": "What is machine learning?\nDiscipline where we define program not by writing it ourself but by allowing it to learn from data and experience. ### What is deep learning? - A general & modern discipline of ML Deep learning is a computer technique to extract and transform data. It is based on multiple layer of neural network and these network are trained to minimize error and maximize accuracy."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#arthur-samuel-ibms-research",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#arthur-samuel-ibms-research",
    "title": "FastAI Lecture 01",
    "section": "Arthur Samuel, IBM’s research",
    "text": "Arthur Samuel, IBM’s research\n\n“Artificial Intelligence: A frontier of Automation”\nProgramming a computer for complex computation like recognition is difficult task not because of complexity in the computer but because of the need specify every minute step of the process in detail.\nIdea: Instead of telling computer each step require to solve; we show examples f problems to solve and let it figure out how to solve it.\nThis was effective in his checker playing program which learned to play by learning from experience.\n\n\n[!idea] Breaking down his idea Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would ‘learn’ from it’s experience."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#getting-into-details-about-samuels-idea-for-ml",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#getting-into-details-about-samuels-idea-for-ml",
    "title": "FastAI Lecture 01",
    "section": "Getting into details about Samuel’s Idea for ML",
    "text": "Getting into details about Samuel’s Idea for ML\n\nWeights / Model Parameters\n\nWeights are variables and weights assignment is a particular values in those variables.\nWeights are additional values which define how the program will operate.\nAltering the weights can change the model’s behavior.\n\nflowchart LR\n    I[Inputs] --&gt;M{Model} \n    W(Weights) --&gt; M\n    M --&gt;F[Resut]\nContextual Example: different weights in checker program would result in different checkers playing strategies ### Actual Performance - Actual performance in checker playing program would be how well does the program plays ### Automatic means of testing - Set two checker playing models to play against each other and see which one is winning ### Mechanism for altering the weight assignment so as to maximize the performance - Difference in the weights of the winning model and losing model; and adjusting the weights little further in the winning direction\n\n\n\n\n\n  Machine programmed to learn from experience  \n\nmodel\n\n    architecture   \n\npredictions\n\n predictions   \n\nmodel-&gt;predictions\n\n    \n\ninputs\n\n inputs   \n\ninputs-&gt;model\n\n    \n\nloss\n\n loss   \n\npredictions-&gt;loss\n\n    \n\nparameters\n\n parameters   \n\nparameters-&gt;model\n\n    \n\nlabels\n\n labels   \n\nlabels-&gt;loss\n\n    \n\nloss-&gt;parameters\n\n  update  \n\n\n\n\n\n\n\nOnce Model has been trained\nWe pack the parameters/weights as being part of the model as we won’t be changing that. The model used for prediction is know as Inference\n\n\n\n\n\n  Inference (Trained Model + Fixed Weight)  \n\nInference\n\n    Inference   \n\npredictions\n\n predictions   \n\nInference-&gt;predictions\n\n    \n\ninputs\n\n inputs   \n\ninputs-&gt;Inference"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#neural-networks",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#neural-networks",
    "title": "FastAI Lecture 01",
    "section": "Neural Networks",
    "text": "Neural Networks\n\nThese are Universal Function which can solve any problem just by varying its weights.\nIf you regard Neural network as a mathematical function, it turns out to be a extremely variable function depending on its weights.\nA mathematical proof called the universal approximation theorem shows that this function(NN) can solve any problem to any level of accuracy in theory.\n\nThe fact that neural networks are so flexible means that, in practice, they are often a suitable kind of model, and you can focus your effort on the process of training them—that is, of finding good weight assignments. ### Search for mechanism which can automatically update the weights according to problem Stochastic gradient descent(SGD)\nNeural networks are special because they are highly flexible, which means they can solve an unusually wide range of problems just by finding the right weights. And SGD provides us a way to find those values automatically!\n\n[!TIP] NN Is particular kind of ML model which fits into the definition of Samuel’s ML program.\n\n\n\n\n\n\n\n\nCase : Cat vs Dog Classifier\nSamuel’s Machine Learning Model\n\n\n\n\nInput Images\nInputs\n\n\nWeights in neural network\nWeights\n\n\nResult (‘Dog’/‘Cat’)\nResult\n\n\nActual Performance at predicting the correct answer\nAutomatic means of testing effectiveness of current weights\n\n\nUpdating weights\nSGD mechanism of updating weights"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#common-terms-and-its-meanings",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#common-terms-and-its-meanings",
    "title": "FastAI Lecture 01",
    "section": "Common Terms and its meanings",
    "text": "Common Terms and its meanings\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nLabel\nThe data that we’re trying to predict, such as “dog” or “cat”\n\n\nArchitecture/Model\nThe functional template of the model that we’re trying to fit; the actual mathematical function that we’re passing the input data and parameters to.\n\n\nParameters\nThe values in the model that change what task it can do, and are updated through model training\n\n\nFit/Trainaka Training model from Scratch\nUpdate the parameters of the model such that the predictions of the model using the input data match the target labelsUsually starts with random weights\n\n\nPretrained model\nA model that has already been trained, generally using a large dataset, and will be fine-tuned\n\n\nFine-tune\nUpdate a pretrained model for a different task (Transfer Learning)Uses pretrained model as starting point\n\n\nModel/Inference\nThe combination of the architecture with a particular set of parameters\n\n\nEpoch\nOne complete pass through the input data\n\n\nLoss\nA measure of performance of the model is, chosen to drive training via SGD\n\n\nMetric\nA measurement of how good the model is, using the validation set, chosen for human consumption\n\n\nValidation set\nA set of data held out from training, used only for measuring how good the model is\n\n\nTraining set\nThe data used for fitting the model; does not include any data from the validation set\n\n\nOverfitting\nTraining a model in such a way that it remembers specific features of the input data, rather than generalizing well to data not seen during training\n\n\nCNN\nConvolutional neural network; a type of neural network that works particularly well for computer vision tasks\n\n\nClassification\nA classification Model is used to predict a class or category.\n\n\nRegression\nA regression model is used to predict one or more numeric quantities, such as temperature or a location."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#feedback-lop",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#feedback-lop",
    "title": "FastAI Lecture 01",
    "section": "Feedback lop",
    "text": "Feedback lop\n\nFeedback loop is the process and outcomes which tends to make model bias towards particular label. Once model starts using it’s initial outcome for training purpose it tends to make prediction which are aligned to new data which in turns creates more prediction for that label This loop is know as positive feedback loop and predictive policing, conspiracy theorist are good example for it."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#overfitting",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#overfitting",
    "title": "FastAI Lecture 01",
    "section": "Overfitting",
    "text": "Overfitting\n\nEven when model has not fully memorized all data, while training it may have memorized certain parts of it.\nAs a result, the longer you train for, the better your accuracy will get on training set and for a while the accuracy will improve in validation set.\nEventually accuracy of validation set will start to get worse as model will set to memorize the training set, rather than finding generalizable underlying patterns in the data.\nThis process is know as overfitting."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#metric",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#metric",
    "title": "FastAI Lecture 01",
    "section": "Metric",
    "text": "Metric\n metric is a function that measures the quality of the model’s predictions using the validation set, and will be printed at the end of each epoch. In this case, we’re using error_rate, which is a function provided by fastai that does just what it says: tells you what percentage of images in the validation set are being classified incorrectly.\n\nWhy Loss isn’t a metric?\nThe concept of a metric may remind you of loss, but there is an important distinction. The entire purpose of loss is to define a “measure of performance” that the training system can use to update weights automatically. In other words, a good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#transfer-learning",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#transfer-learning",
    "title": "FastAI Lecture 01",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nWhen using a pretrained model, the last layer, specific to the original task, is replaced with a new head for the target dataset. Pretrained models are crucial for training accurate models quickly and efficiently with limited data and resources. However, transfer learning, using pretrained models for different tasks, is under-studied, limiting its application in some domains like medicine. Despite its potential, the importance of pretrained models is often overlooked in academic deep learning."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#fit-vs-fine-tune",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#fit-vs-fine-tune",
    "title": "FastAI Lecture 01",
    "section": "Fit vs Fine Tune",
    "text": "Fit vs Fine Tune\nThe architecture only describes a template for a mathematical function; it doesn’t actually do anything until we provide values for the millions of parameters it contains.\nThis is the key to deep learning—determining how to fit the parameters of a model to get it to solve your problem. In order to fit a model, we have to provide at least one piece of information: how many times to look at each image (known as number of epochs). The number of epochs you select will largely depend on how much time you have available, and how long you find it takes in practice to fit your model. If you select a number that is too small, you can always train for more epochs later.\nBut why is the method called fine_tune, and not fit? fastai actually does have a method called fit, which does indeed fit a model (i.e. look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels). But in this case, we’ve started with a pretrained model, and we don’t want to throw away all those capabilities that it already has. As you’ll learn in this book, there are some important tricks to adapt a pretrained model for a new dataset—a process called fine-tuning.\n\nDeep learning architecture is a template for a function, requiring parameter values to be effective.\nThe key in deep learning is fitting the model parameters to solve a specific problem.\nThe number of epochs (how many times to look at each image) is crucial for model fitting.\nfine_tune is used instead of fit when starting with a pretrained model to retain its capabilities.\nFine-tuning involves adapting a pretrained model for a new dataset, preserving its learned features.\n\nCNN: Current state of the art approach to creating computer vision models."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 01.html#fine-tune-process",
    "href": "posts/Notes/FastAI/FastAI Lecture 01.html#fine-tune-process",
    "title": "FastAI Lecture 01",
    "section": "Fine Tune Process",
    "text": "Fine Tune Process\nWhen you use the fine_tune method, fastai will use these tricks for you. There are a few parameters you can set (which we’ll discuss later), but in the default form shown here, it does two steps:\n\nUse one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.\nUse the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which, as we’ll see, generally don’t require many changes from the pretrained weights).\n\nThe head of a model is the part that is newly added to be specific to the new dataset. An epoch is one complete pass through the dataset. After calling fit, the results after each epoch are printed, showing the epoch number, the training and validation set losses (the “measure of performance” used for training the model), and any metrics you’ve requested (error rate, in this case).\n\nGo over Jargon recap in fastbook lesson 1\n\nReduce batch size if you are hitting cuda out of memory"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 03.html",
    "href": "posts/Notes/FastAI/FastAI Lecture 03.html",
    "title": "FastAI Lecture 02",
    "section": "",
    "text": "Arrays/Tensor\nBroadcasting\nSGD\nLoss function\nRole of mini batches\ndescribe the math that a basic neural network is actually doing"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 03.html#goal",
    "href": "posts/Notes/FastAI/FastAI Lecture 03.html#goal",
    "title": "FastAI Lecture 02",
    "section": "",
    "text": "Arrays/Tensor\nBroadcasting\nSGD\nLoss function\nRole of mini batches\ndescribe the math that a basic neural network is actually doing"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Lecture 03.html#calculating-loss",
    "href": "posts/Notes/FastAI/FastAI Lecture 03.html#calculating-loss",
    "title": "FastAI Lecture 02",
    "section": "Calculating Loss",
    "text": "Calculating Loss\n\nWhat are derivatives?\nDerivatives define the rate of change for the particular function at that particular point of parameter. &gt; In machine learning key is to know how to change the parameter (weights) of a function to reduce the loss. We can use derivatives as it gives us the understanding of change which would take place on altering weights. Calculus provides derivatives which can help us create gradients of the function\n\n\nCalculating derivatives for weights in NN\nOur function has large number of weights. We calculate the derivative for one weight while we consider rest as constant and repeat the same process.\n &gt; in deep learning, “gradients” usually means the value of a function’s derivative at a particular argument value whereas in math the derivative gives us another function(rate)\npytorch provides requires_grad_() which tags the variable and keeps track to calculate derivative when passed in a function\ndef f(x): return x**2\n\nxt = tensor(3.).requires_grad_()\n\n## Calculating function with the value \nyt = f(xt)\nyt\n&gt;&gt;tensor(9., grad_fn=&lt;PowBackward0&gt;)\n\n## Asking pytorch to calculate gradient for us\nyt.backwards()\n# The \"backward\" here refers to _backpropagation_, which is the name given to the process of calculating the derivative of each layer.\n\nxt.grad\n&gt;&gt; tensor(6.)\nderivative of f(x) = x^2 is 2*x We found the same value with the xt.grad (gradient)\n\nThe gradients only tell us the slope of our function, they don’t actually tell us exactly how far to adjust the parameters. But it gives us some idea of how far; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Snippets.html",
    "href": "posts/Notes/FastAI/FastAI Snippets.html",
    "title": "FastAI Snippets 01",
    "section": "",
    "text": "Page contains some of the common and usefull snippets. Pathlib, parallel processing."
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Snippets.html#path",
    "href": "posts/Notes/FastAI/FastAI Snippets.html#path",
    "title": "FastAI Snippets 01",
    "section": "Path",
    "text": "Path\nfrom pathlib import path\npath = Path() #Current Dir\n#path = Path('Content') #Relative Dir\n#path = Path('NB/content') #Absolute Dir"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Snippets.html#add-recent-models",
    "href": "posts/Notes/FastAI/FastAI Snippets.html#add-recent-models",
    "title": "FastAI Snippets 01",
    "section": "Add recent models",
    "text": "Add recent models\n!pip install timm\nimport timm\ntimm.list_models('convnext*) # Prints available model"
  },
  {
    "objectID": "posts/Notes/FastAI/FastAI Snippets.html#fine_tune",
    "href": "posts/Notes/FastAI/FastAI Snippets.html#fine_tune",
    "title": "FastAI Snippets 01",
    "section": "Fine_Tune:",
    "text": "Fine_Tune:\nFine Tune Freezes the weights of all layers except the last layers. Calls fit on the last layer Decreases the learning rate Unfreezes the model Fit for number of epoch specified\n\nhalf precision floating points (less precise but fast) supported on latest gpus .to_fp16() (method for learnerer)\n\n\n[!info] Learning rate and lr_find() : Is used to find good learning rate"
  },
  {
    "objectID": "posts/QuickStart.html",
    "href": "posts/QuickStart.html",
    "title": "Quarto ipynb blog test",
    "section": "",
    "text": "Quarto JP Notebook\n\nfrom fastai.vision.all import *\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nUsing device: cuda\n\n\n\npath = untar_data(URLs.PETS)/'images'\n\nDownloading a new version of this dataset...\n\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 18:21&lt;00:00]\n    \n    \n\n\n\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /home/agastya/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████████████████████████████████████████████| 83.3M/83.3M [00:14&lt;00:00, 6.18MB/s]\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.166146\n0.011790\n0.004060\n00:42\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.053903\n0.018103\n0.003383\n00:47"
  }
]