---
title: "FastAI Lecture 02"
author: "Agastya Patel"
date: "2024-01-11"
date-modified: "2023-12-31"
categories: [Notes, FastAI, History, Neural Network, Theory]
---

DataLoaders
- Dataloaders is a container which contains the data (train and validation) we want to use to for our machine learning model; it organises the data.
- DataLoaders class is the class that passes the data to the fastai model. It is essentially a class that required Dataloader objects (usually for train and validation subset)

item_tfms: Our images are all different sizes, and this is a problem for deep learning: we don't feed the model one image at a time but several of them (what we call a mini-batch). To group them in a big array (usually called a tensor) that is going to go through our model, they all need to be of the same size. So, we need to add a transform which will resize these images to the same size. Item transforms are pieces of code that run on each individual item, whether it be an image, category, or so forth. fastai includes many predefined transforms; we use the Resize transform here:


DataBlock: This command has given us a `DataBlock` object. This is like a _template_ for creating a `DataLoaders`. We still need to tell fastai the actual source of our data—in this case, the path where the images can be found

A `DataLoaders` includes validation and training `DataLoader`s. `DataLoader` is a class that provides batches of a few items at a time to the GPU. We'll be learning a lot more about this class in the next chapter. When you loop through a `DataLoader` fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the `show_batch` method on a `DataLoader`: `dls.valid.show_batch(max_n=4, nrows=1)`
dls is a dataloaders
dls.valid and dls.train is dataloader

## Cropping
- Resize(size): crops the image to fit a square of the size requested, using the full width or height. 
- ResizeMethod.Squish: can result in loosing of some detail as images gets squished or stretched.
- ResizeMethod.Pad, pad_mode='zero': Fits the image in square with black padding.
These approaches are unoptimized and result in lower accurate models.
We can train our model on random parts of image during each training round (EPOCH), making it adaptable to different features, similar to how photos can have different perspective.
It helps the model to generalize the feature and learn the basic concept of object
- RandomResizedCrop(size, min_scale=0.3): Creates random zoom images during epoch in vram 
`dls.train.show_batch(max_n=4, nrows=1, unique=True)`
We used `unique=True` to have the same image repeated with different versions of this `RandomResizedCrop` transform. This is a specific example of a more general technique, called data augmentation.

## Data Augmentation
With resize our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time.